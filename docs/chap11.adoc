== Flink Table API & SQL

Table API是流处理和批处理通用的关系型API，Table API可以基于流输入或者批输入来运行而不需要进行任何修改。Table API是SQL语言的超集并专门为Apache Flink设计的，Table API是Scala 和Java语言集成式的API。与常规SQL语言中将查询指定为字符串不同，Table API查询是以Java或Scala中的语言嵌入样式来定义的，具有IDE支持如:自动完成和语法检测。

=== 导入依赖

根据使用的语言不同（Java，Scala），以下依赖选择一个使用。

[source,xml]
----
<!-- Either... -->
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-table-api-java-bridge_2.11</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
<!-- or... -->
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-table-api-scala-bridge_2.11</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
----

如果想兼容以前写的代码，可以使用下面的第一个依赖；如果想要使用阿里巴巴开发的一些新特性（TopN之类的），可以选择第二个依赖。

[source,xml]
----
<!-- Either... (for the old planner that was available before Flink 1.9) -->
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-table-planner_2.11</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
<!-- or.. (for the new Blink planner) -->
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-table-planner-blink_2.11</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
----

如果想要实现一些用户自定义函数（UDF），添加下面的依赖

[source,xml]
----
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-table-common</artifactId>
  <version>1.10.0</version>
  <scope>provided</scope>
</dependency>
----

=== 简单了解Table API

[source,scala]
----
def main(args: Array[String]): Unit = {
  val env = StreamExecutionEnvironment.getExecutionEnvironment
  env.setParallelism(1)
  val inputStream = env.readTextFile("sensor.txt")
  val dataStream = inputStream
    .map(data => {
      val dataArray = data.split(",")
      SensorReading(dataArray(0).trim, dataArray(1).trim.toLong, dataArray(2).trim.toDouble)
    })
  val settings: EnvironmentSettings = EnvironmentSettings
    .newInstance()
    .useOldPlanner()
    .inStreamingMode()
    .build()
  val tableEnv: StreamTableEnvironment = StreamTableEnvironment.create(env, settings)
  // 从一条流创建一张表
  val dataTable: Table = tableEnv.fromDataStream(dataStream)
  // 从表里选取特定的数据
  val selectedTable: Table = dataTable
    .select('id, 'temperature)
    .filter("id = 'sensor_1'")
  val selectedStream: DataStream[(String, Double)] = selectedTable
    .toAppendStream[(String, Double)]
  selectedStream.print()
  env.execute("table test")
}
----

==== 动态表

如果流中的数据类型是case class可以直接根据case class的结构生成table

[source,scala]
----
tableEnv.fromDataStream(dataStream)
----

或者根据字段顺序单独命名

[source,scala]
----
tableEnv.fromDataStream(dataStream, 'id, 'timestamp)
----

最后的动态表可以转换为流进行输出

[source,scala]
----
table.toAppendStream[(String,String)]
----

==== 字段

用一个单引号放到字段前面来标识字段名, 如 'name , 'id , 'amount 等

=== Table API 的窗口聚合操作

==== 通过一个例子了解TableAPI

[source,scala]
----
// 统计每10秒中每个传感器温度值的个数
def main(args: Array[String]): Unit = {
  val env = StreamExecutionEnvironment.getExecutionEnvironment
  env.setParallelism(1)
  env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
  val inputStream = env.readTextFile("..\\sensor.txt")
  val dataStream = inputStream
    .map(data => {
      val dataArray = data.split(",")
      SensorReading(dataArray(0).trim, dataArray(1).trim.toLong, dataArray(2).trim.toDouble)
    })
    .assignTimestampsAndWatermarks(
      new BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.seconds(1)) {
        override def extractTimestamp(element: SensorReading): Long = element.timestamp * 1000L
      }
    )
  // 基于env创建 tableEnv
  val settings: EnvironmentSettings = EnvironmentSettings
    .newInstance()
    .useOldPlanner()
    .inStreamingMode()
    .build()
  val tableEnv: StreamTableEnvironment = StreamTableEnvironment.create(env, settings)
  // 从一条流创建一张表，按照字段去定义，并指定事件时间的时间字段
  val dataTable: Table = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'ts.rowtime)
  // 按照时间开窗聚合统计
  val resultTable: Table = dataTable
    .window(Tumble over 10.seconds on 'ts as 'tw )
    .groupBy('id, 'tw)
    .select('id, 'id.count)
  val selectedStream: DataStream[(Boolean, (String, Long))] = resultTable
    .toRetractStream[(String, Long)]
  selectedStream.print()
  env.execute("table window test")
}
----

==== 关于group by

1. 如果了使用 groupby，table转换为流的时候只能用toRetractDstream

[source,scala]
----
val dataStream: DataStream[(Boolean, (String, Long))] = table.toRetractStream[(String,Long)]
----

2. toRetractDstream得到的第一个boolean型字段标识true就是最新的数据(Insert)，false表示过期老数据(Delete)

[source,scala]
----
val dataStream: DataStream[(Boolean, (String, Long))] = table.toRetractStream[(String,Long)]
dataStream.filter(_._1).print()
----

3. 如果使用的api包括时间窗口，那么窗口的字段必须出现在groupBy中。

[source,scala]
----
val resultTable: Table = dataTable
  .window( Tumble over 10.seconds on 'ts as 'tw )
  .groupBy('id, 'tw)
  .select('id, 'id.count)
----

==== 关于时间窗口

1. 用到时间窗口，必须提前声明时间字段，如果是Processing Time直接在创建动态表时进行追加就可以。

[source,scala]
----
val dataTable: Table = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'ps.proctime)
----

2. 如果是EventTime要在创建动态表时声明

[source,scala]
----
val dataTable: Table = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'ts.rowtime)
----

3. 滚动窗口可以使用Tumble over 10000.millis on来表示

[source,scala]
----
val resultTable: Table = dataTable
  .window( Tumble over 10.seconds on 'ts as 'tw)
  .groupBy('id, 'tw)
  .select('id, 'id.count)
----

=== SQL如何编写

[source,scala]
----
// 统计每10秒中每个传感器温度值的个数
def main(args: Array[String]): Unit = {
  val env = StreamExecutionEnvironment.getExecutionEnvironment
  env.setParallelism(1)
  env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
  val inputStream = env.readTextFile("sensor.txt")
  val dataStream = inputStream
    .map(data => {
      val dataArray = data.split(",")
      SensorReading(dataArray(0).trim, dataArray(1).trim.toLong, dataArray(2).trim.toDouble)
    })
    .assignTimestampsAndWatermarks(
      new BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.seconds(1)) {
        override def extractTimestamp(element: SensorReading): Long = element.timestamp * 1000L
      }
    )
  // 基于env创建 tableEnv
  val settings: EnvironmentSettings = EnvironmentSettings
    .newInstance()
    .useOldPlanner()
    .inStreamingMode()
    .build()
  val tableEnv: StreamTableEnvironment = StreamTableEnvironment.create(env, settings)
  // 从一条流创建一张表，按照字段去定义，并指定事件时间的时间字段
  val dataTable: Table = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'ts.rowtime)
  // 直接写sql完成开窗统计
  val resultSqlTable: Table = tableEnv.sqlQuery("select id, count(id) from " + dataTable + " group by id, tumble(ts, interval '15' second)")
  val selectedStream: DataStream[(Boolean, (String, Long))] = resultSqlTable.toRetractStream[(String, Long)]
  selectedStream.print()
  env.execute("table window test")
}
----

=== 使用Table API结合SQL实现TopN需求

[source,scala]
----
package com.atguigu.project.topnhotitems

import java.sql.Timestamp

import com.atguigu.project.util.UserBehavior
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.api.scala._
import org.apache.flink.table.api.{EnvironmentSettings, Tumble}
import org.apache.flink.table.api.scala._

object HotItemsTable {

  def main(args: Array[String]): Unit = {

    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 有关Blink的配置，样板代码
    val settings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    // 创建流式表的环境
    val tEnv = StreamTableEnvironment.create(env, settings)
    env.setParallelism(1)
    // 使用事件时间
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    // 过滤出pv事件，并抽取时间戳
    val stream = env
      .readTextFile("/home/parallels/flink-tutorial/Flink0830Tutorial/src/main/resources/UserBehavior.csv")
      .map(line => {
        val arr = line.split(",")
        UserBehavior(arr(0).toLong, arr(1).toLong, arr(2).toInt, arr(3), arr(4).toLong * 1000)
      })
      .filter(_.behavior == "pv")
      .assignAscendingTimestamps(_.timestamp)

    // 从流中提取两个字段，时间戳；itemId，组成一张表
    val table = tEnv.fromDataStream(stream, 'timestamp.rowtime, 'itemId)
    val t = table
      .window(Tumble over 60.minutes on 'timestamp as 'w) // 一小时滚动窗口
      .groupBy('itemId, 'w)                               // 根据itemId和窗口进行分组
      .aggregate('itemId.count as 'icount)                // 对itemId进行计数
      .select('itemId, 'icount, 'w.end as 'windowEnd)     // 查询三个字段
      .toAppendStream[(Long, Long, Timestamp)]            // 转换成DataStream

    // 创建临时表
    tEnv.createTemporaryView("topn", t, 'itemId, 'icount, 'windowEnd)

    // topN查询，Blink支持的特性
    val result = tEnv.sqlQuery(
      """
        |SELECT *
        |FROM (
        |    SELECT *,
        |        ROW_NUMBER() OVER (PARTITION BY windowEnd ORDER BY icount DESC) as row_num
        |    FROM topn)
        |WHERE row_num <= 5
        |""".stripMargin
    )
    // 使用toRetractStream转换成DataStream，用来实时更新排行榜
    // true代表insert, false代表delete
    result.toRetractStream[(Long, Long, Timestamp, Long)].print()

    env.execute()
  }
}
----

=== 只使用Flink SQL实现TopN需求

代码

[source,scala]
----
package com.atguigu.project.topnhotitems

import java.sql.Timestamp

import com.atguigu.project.util.UserBehavior
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.api.scala._
import org.apache.flink.table.api.{EnvironmentSettings, Tumble}
import org.apache.flink.table.api.scala._

object HotItemsSQL {

  def main(args: Array[String]): Unit = {

    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val settings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tEnv = StreamTableEnvironment.create(env, settings)
    env.setParallelism(1)
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    val stream = env
      .readTextFile("/home/parallels/flink-tutorial/Flink0830Tutorial/src/main/resources/UserBehavior.csv")
      .map(line => {
        val arr = line.split(",")
        UserBehavior(arr(0).toLong, arr(1).toLong, arr(2).toInt, arr(3), arr(4).toLong * 1000)
      })
      .filter(_.behavior == "pv")
      .assignAscendingTimestamps(_.timestamp)

    tEnv.createTemporaryView("t", stream, 'itemId, 'timestamp.rowtime as 'ts)

    val result = tEnv.sqlQuery(
      """
        |SELECT *
        |FROM (
        |    SELECT *,
        |        ROW_NUMBER() OVER (PARTITION BY windowEnd ORDER BY icount DESC) as row_num
        |    FROM (SELECT count(itemId) as icount, TUMBLE_START(ts, INTERVAL '1' HOUR) as windowEnd FROM t GROUP BY TUMBLE(ts, INTERVAL '1' HOUR), itemId) topn)
        |WHERE row_num <= 5
        |""".stripMargin
    )
    result.toRetractStream[(Long, Timestamp, Long)].print()

    env.execute()
  }
}
----

=== 用户自定义函数

==== Scalar Functions

例子

[source,scala]
----
package com.atguigu.course

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.api.scala._
import org.apache.flink.table.api.{EnvironmentSettings, Tumble}
import org.apache.flink.table.api.scala._
import org.apache.flink.table.functions.ScalarFunction

object TableUDFExample1 {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val settings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tEnv = StreamTableEnvironment.create(env, settings)
    env.setParallelism(1)
    val stream = env.addSource(new SensorSource)
    val hashCode = new HashCode(10)
    tEnv.registerFunction("hashCode", new HashCode(10))
    val table = tEnv.fromDataStream(stream, 'id)
    // table api 写法
    table
      .select('id, hashCode('id))
      .toAppendStream[(String, Int)]
      .print()

    // sql 写法
    tEnv.createTemporaryView("t", table, 'id)
    tEnv
      .sqlQuery("SELECT id, hashCode(id) FROM t")
      .toAppendStream[(String, Int)]
      .print()

    env.execute()
  }

  class HashCode(factor: Int) extends ScalarFunction {
    def eval(s: String): Int = {
      s.hashCode() * factor
    }
  }
  
}
----

==== Table Functions

[source,scala]
----
package com.atguigu.course

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.api.scala._
import org.apache.flink.table.api.{EnvironmentSettings, Tumble}
import org.apache.flink.table.api.scala._
import org.apache.flink.table.functions.TableFunction

object TableUDFExample2 {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val settings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tEnv = StreamTableEnvironment.create(env, settings)
    val split = new Split("#")
    env.setParallelism(1)
    val stream = env.fromElements(
      "hello#world"
    )
    val table = tEnv.fromDataStream(stream, 's)
    // table api 写法
    table
      .joinLateral(split('s) as ('word, 'length))
      .select('s, 'word, 'length)
      .toAppendStream[(String, String, Long)]
      .print()
    table
      .leftOuterJoinLateral(split('s) as ('word, 'length))
      .select('s, 'word, 'length)
      .toAppendStream[(String, String, Long)]
      .print()

    // sql 写法
    tEnv.registerFunction("split", new Split("#"))

    tEnv.createTemporaryView("t", table, 's)

    // Use the table function in SQL with LATERAL and TABLE keywords.
    // CROSS JOIN a table function (equivalent to "join" in Table API)
    tEnv
      .sqlQuery("SELECT s, word, length FROM t, LATERAL TABLE(split(s)) as T(word, length)")
      .toAppendStream[(String, String, Int)]
      .print()
    // LEFT JOIN a table function (equivalent to "leftOuterJoin" in Table API)
    tEnv
      .sqlQuery("SELECT s, word, length FROM t LEFT JOIN LATERAL TABLE(split(s)) as T(word, length) ON TRUE")
      .toAppendStream[(String, String, Int)]
      .print()

    env.execute()

  }
  class Split(separator: String) extends TableFunction[(String, Int)] {
    def eval(str: String): Unit = {
      // use collect(...) to emit a row.
      str.split(separator).foreach(x => collect((x, x.length)))
    }
  }
}
----

==== Aggregation Functions

例子

[source,scala]
----
package com.atguigu.course

import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.flink.api.java.typeutils.{RowTypeInfo, TupleTypeInfo}
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.api.scala._
import org.apache.flink.table.api.{EnvironmentSettings, Tumble}
import org.apache.flink.table.api.scala._
import org.apache.flink.table.functions.AggregateFunction
import org.apache.flink.table.api.Types
import org.apache.flink.types.Row

object TableUDFExample3 {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val settings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tEnv = StreamTableEnvironment.create(env, settings)
    val myAggFunc = new MyMinMax
    env.setParallelism(1)
    val stream = env.fromElements(
      (1, -1),
      (1, 2)
    )
    val table = tEnv.fromDataStream(stream, 'key, 'a)
    table
      .groupBy('key)
      .aggregate(myAggFunc('a) as ('x, 'y))
      .select('key, 'x, 'y)
      .toRetractStream[(Long, Long, Long)]
      .print()
    
    env.execute()
  }

  case class MyMinMaxAcc(var min: Int, var max: Int)

  class MyMinMax extends AggregateFunction[Row, MyMinMaxAcc] {

    def accumulate(acc: MyMinMaxAcc, value: Int): Unit = {
      if (value < acc.min) {
        acc.min = value
      }
      if (value > acc.max) {
        acc.max = value
      }
    }

    override def createAccumulator(): MyMinMaxAcc = MyMinMaxAcc(0, 0)

    def resetAccumulator(acc: MyMinMaxAcc): Unit = {
      acc.min = 0
      acc.max = 0
    }

    override def getValue(acc: MyMinMaxAcc): Row = {
      Row.of(Integer.valueOf(acc.min), Integer.valueOf(acc.max))
    }

    override def getResultType: TypeInformation[Row] = {
      new RowTypeInfo(Types.INT, Types.INT)
    }
  }
}
----

==== Table Aggregation Functions

例子

[source,scala]
----
package com.atguigu.course

import java.lang.{Integer => JInteger}

import org.apache.flink.table.functions.TableAggregateFunction
import org.apache.flink.api.scala._
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.table.api.EnvironmentSettings
import org.apache.flink.table.api.scala._
import org.apache.flink.api.java.tuple.{Tuple2 => JTuple2}
import java.lang.{Iterable => JIterable}

import org.apache.flink.util.Collector

object TableUDFExample4 {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val settings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tEnv = StreamTableEnvironment.create(env, settings)
    env.setParallelism(1)
    val stream = env.fromElements(
      (1, -1),
      (1, 2),
      (1, 0),
      (1, 5),
      (1, 4)
    )
    val top2 = new Top2
    val table = tEnv.fromDataStream(stream, 'key, 'a)
    table
      .groupBy('key)
      .flatAggregate(top2('a) as ('v, 'rank))
      .select('key, 'v, 'rank)
      .toRetractStream[(Long, Long, Long)]
      .print()

    env.execute()
  }

  /**
   * Accumulator for top2.
   */
  class Top2Accum {
    var first: JInteger = _
    var second: JInteger = _
  }

  /**
   * The top2 user-defined table aggregate function.
   */
  class Top2 extends TableAggregateFunction[JTuple2[JInteger, JInteger], Top2Accum] {

    override def createAccumulator(): Top2Accum = {
      val acc = new Top2Accum
      acc.first = Int.MinValue
      acc.second = Int.MinValue
      acc
    }

    def accumulate(acc: Top2Accum, v: Int) {
      if (v > acc.first) {
        acc.second = acc.first
        acc.first = v
      } else if (v > acc.second) {
        acc.second = v
      }
    }

    def merge(acc: Top2Accum, its: JIterable[Top2Accum]): Unit = {
      val iter = its.iterator()
      while (iter.hasNext) {
        val top2 = iter.next()
        accumulate(acc, top2.first)
        accumulate(acc, top2.second)
      }
    }

    def emitValue(acc: Top2Accum, out: Collector[JTuple2[JInteger, JInteger]]): Unit = {
      // emit the value and rank
      if (acc.first != Int.MinValue) {
        out.collect(JTuple2.of(acc.first, 1))
      }
      if (acc.second != Int.MinValue) {
        out.collect(JTuple2.of(acc.second, 2))
      }
    }
  }
}
----